Here’s a tight pre-read of **“A Definition of AGI”**:

**Objective**
Define AGI in a concrete, testable way and provide a framework to *measure* progress toward it, rather than letting the term be a moving goalpost. ([agidefinition.ai][1])

**Key Findings**

* **Definition:** “AGI is an AI that can match or exceed the cognitive versatility and proficiency of a well-educated adult.” ([agidefinition.ai][1])
* **Jagged capability profile:** Today’s models are strong in knowledge-heavy areas but weak in foundational cognition—especially **long-term memory storage**. ([agidefinition.ai][1])
* **Headline scores (illustrative):** GPT-4 ≈ **27%**, GPT-5 ≈ **58%** on the proposed AGI scale—showing progress but a sizable gap to AGI. ([agidefinition.ai][1])

**Methodology**

* Ground the evaluation in **Cattell-Horn-Carroll (CHC) theory**, a leading human cognitive model. ([agidefinition.ai][1])
* Assess systems across **10 core cognitive domains** (e.g., reasoning, memory, perception) using adapted **human psychometric batteries**; compute a normalized “AGI Score.” ([agidefinition.ai][1])
* Domains are **equally weighted (10% each)** to emphasize breadth. ([agidefinition.ai][1])

**Significance**

* Establishes a **standardized yardstick** for AGI progress, clarifying how far current systems are from human-level generality and where the bottlenecks are (notably memory). ([newsletter.safe.ai][2])

**Limitations (from the paper’s setup & overview)**

* **Design choice:** Equal domain weighting may not reflect real-world importance of abilities (an explicit modeling choice noted on the site). ([agidefinition.ai][1])
* **Proxy measurements:** Reliance on adapted human psychometrics means results depend on how well those tests transfer to AI systems (an inherent caveat of the approach as described in the overview/news write-up). ([newsletter.safe.ai][2])

**Future Directions**

* **Fix long-term memory:** Authors highlight **long-term memory** as the critical bottleneck; reducing reliance on giant context windows and building persistent memory should be a priority. ([newsletter.safe.ai][2])
* **Broaden & refine evaluation:** Continue refining domain coverage and test adaptations as models evolve, keeping the metric stable while improving fidelity to general cognition. ([agidefinition.ai][1])


[1]: https://www.agidefinition.ai/ "A Definition of AGI"
[2]: https://newsletter.safe.ai/p/ai-safety-newsletter-63-new-agi-definition?action=share&utm_content=share&utm_medium=email&utm_source=substack "AI Safety Newsletter #64: New AGI Definition and Senate Bill Would Establish Liability for AI Harms"

